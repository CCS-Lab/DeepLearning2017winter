{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week5_cnn.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9mWjVx02eTUG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Week5**\n",
        "# **Convolutional Neural Network**"
      ]
    },
    {
      "metadata": {
        "id": "Fyk_ABPgeTUH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![cnn](http://parse.ele.tue.nl/cluster/2/CNNArchitecture.jpg)"
      ]
    },
    {
      "metadata": {
        "id": "E9kkMviZeTUI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Repeat lab video\n",
        "#### Simple Convolution Layer"
      ]
    },
    {
      "metadata": {
        "id": "K04SY8wAeTUJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zrcpNyyIeTUO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            },
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "054444ca-b6f1-4905-f6eb-8112ebcead77",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018029200,
          "user_tz": -540,
          "elapsed": 1118,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession()\n",
        "image = np.array([[[[1],[2],[3]],\n",
        "                   [[4],[5],[6]], \n",
        "                   [[7],[8],[9]]]], dtype=np.float32)\n",
        "print(image.shape)\n",
        "plt.imshow(image.reshape(3,3), cmap='Greys')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 3, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd6fc5a36d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQkAAAD8CAYAAABkQFF6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd702e40fd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u1kImWrZeTUR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "b599d562-6599-435f-9571-dd4ea8c9e88f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018030156,
          "user_tz": -540,
          "elapsed": 933,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# # print(\"imag:\\n\", image)\n",
        "print(\"image.shape\", image.shape)\n",
        "\n",
        "weight = tf.constant([[[[1.,10.,-1.]],[[1.,10.,-1.]]],\n",
        "                      [[[1.,10.,-1.]],[[1.,10.,-1.]]]])\n",
        "print(\"weight.shape\", weight.shape)\n",
        "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
        "conv2d_img = conv2d.eval()\n",
        "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
        "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
        "for i, one_img in enumerate(conv2d_img):\n",
        "    print(one_img.reshape(3,3))\n",
        "    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image.shape (1, 3, 3, 1)\n",
            "weight.shape (2, 2, 1, 3)\n",
            "conv2d_img.shape (1, 3, 3, 3)\n",
            "[[12. 16.  9.]\n",
            " [24. 28. 15.]\n",
            " [15. 17.  9.]]\n",
            "[[120. 160.  90.]\n",
            " [240. 280. 150.]\n",
            " [150. 170.  90.]]\n",
            "[[-12. -16.  -9.]\n",
            " [-24. -28. -15.]\n",
            " [-15. -17.  -9.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAACBCAYAAAACG0sVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAiMkgaAwChpAAjsv0GqfVPJP9IQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd6fc5a39e8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "mtFjCQ4JeTUU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Max Pooling"
      ]
    },
    {
      "metadata": {
        "id": "RQAaWlmqeTUX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "b30946bb-d1f3-46d0-9ee0-4229ef546a4b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018031006,
          "user_tz": -540,
          "elapsed": 823,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "image = np.array([[[[4],[3]],\n",
        "                    [[2],[1]]]], dtype=np.float32)\n",
        "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 1, 1, 1], padding='SAME')\n",
        "print(pool.shape)\n",
        "print(pool.eval())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 2, 2, 1)\n",
            "[[[[4.]\n",
            "   [3.]]\n",
            "\n",
            "  [[2.]\n",
            "   [1.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xdsitlGYeTUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remember\n",
        "image.shape = [# of image, n, n, color]\n",
        "\n",
        "weight.shape = [n, n, color, # of filter]\n",
        "\n",
        "stride = [1, n, n, 1]\n",
        "\n",
        "ksize = [1, n, n, 1]\n",
        "\n",
        "original image: nXn, \n",
        "stride: 2X2, \n",
        "padding='SAME'\n",
        "=> output: (n/2) X (n/2)"
      ]
    },
    {
      "metadata": {
        "id": "4qoNDgSWeTUa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN MNIST: 99%"
      ]
    },
    {
      "metadata": {
        "id": "F-PDTJGReTUa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5c0d9a44-ef4a-47e2-8db5-f81116b61b81",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018032813,
          "user_tz": -540,
          "elapsed": 1739,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pQUYB9Z0eTUk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "keep_prob = tf.placeholder(tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UpAjiO4QeTUm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "X_img = tf.reshape(X, [-1, 28, 28, 1])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "32QtZ6IjeTUo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L1 = tf.nn.relu(L1)\n",
        "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0hZq2jHPeTUr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L2 = tf.nn.relu(L2)\n",
        "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nUmxAi8XeTUt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "L3 = tf.nn.relu(L3)\n",
        "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], \n",
        "                    strides=[1, 2, 2, 1], padding='SAME')\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2kLTGnkreTUv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([625]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "klLkSYbBeTUy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "logits = tf.matmul(L4, W5) + b5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ppaxTIjIeTU0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "56be61ee-7c99-41fc-a700-1e3802317c20",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018038600,
          "user_tz": -540,
          "elapsed": 847,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-13-8aa1328b7014>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_s4yGj5xeTU5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yAxJ-K19eTU7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 16
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "45362fde-cdc2-45aa-f43d-6266c474caa2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018138425,
          "user_tz": -540,
          "elapsed": 99120,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print('Learning started. It takes sometime.')\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning started. It takes sometime.\n",
            "Epoch: 0001 cost = 0.390806842\n",
            "Epoch: 0002 cost = 0.093335660\n",
            "Epoch: 0003 cost = 0.070024019\n",
            "Epoch: 0004 cost = 0.055735740\n",
            "Epoch: 0005 cost = 0.052725835\n",
            "Epoch: 0006 cost = 0.045113899\n",
            "Epoch: 0007 cost = 0.042455362\n",
            "Epoch: 0008 cost = 0.039459311\n",
            "Epoch: 0009 cost = 0.036316336\n",
            "Epoch: 0010 cost = 0.033372368\n",
            "Epoch: 0011 cost = 0.031189243\n",
            "Epoch: 0012 cost = 0.031500518\n",
            "Epoch: 0013 cost = 0.026956368\n",
            "Epoch: 0014 cost = 0.029344459\n",
            "Epoch: 0015 cost = 0.026344775\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wRyUH4KgeTU-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a21b3568-d7b6-45fa-de3d-f3c340178126",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018140502,
          "user_tz": -540,
          "elapsed": 2042,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ylRV-ly_eTVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Python Class"
      ]
    },
    {
      "metadata": {
        "id": "JANduPD8eTVD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
        "            # for testing\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "            # img 28x28x1 (black/white), Input Layer\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # Convolutional Layer #1 and # Pooling Layer #1\n",
        "            weight1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "            layer1 = tf.nn.conv2d(X_img, weight1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer1 = tf.nn.relu(layer1)\n",
        "            layer1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer1 = tf.nn.dropout(layer1, keep_prob=0.7)\n",
        "\n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            weight2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "            layer2 = tf.nn.conv2d(layer1, weight2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer2 = tf.nn.relu(layer2)\n",
        "            layer2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer2 = tf.nn.dropout(layer2, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            weight3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "            layer3 = tf.nn.conv2d(layer2, weight3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer3 = tf.nn.relu(layer3)\n",
        "            layer3 = tf.nn.max_pool(layer3, ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer3 = tf.nn.dropout(layer3, keep_prob=0.7)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(layer3, [-1, 128 * 4 * 4])\n",
        "            weight4 = tf.get_variable(\"weight4\", shape=[128 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias4 = tf.Variable(tf.random_normal([625]))\n",
        "            layer4 = tf.nn.relu(tf.matmul(flat, weight4) + bias4)\n",
        "            layer4 = tf.nn.dropout(layer4, keep_prob=0.7)\n",
        "            \n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
        "            weight5 = tf.get_variable(\"weight5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias5 = tf.Variable(tf.random_normal([10]))\n",
        "            self.logits = tf.matmul(layer4, weight5) + bias5\n",
        "            \n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits,\n",
        "                             feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy,\n",
        "                             feed_dict={self.X: x_test,\n",
        "                                        self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.training: training})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0eqwBM2YeTVF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ensemble"
      ]
    },
    {
      "metadata": {
        "id": "VVJ18bereTVG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae9c30c8-c793-43d7-9f86-9fd5855829ce",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018142841,
          "user_tz": -540,
          "elapsed": 1604,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "models = []\n",
        "num_models = 2\n",
        "for m in range(num_models):\n",
        "    models.append(Model(sess, \"model\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Started!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J3gAPOJOeTVJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 15
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "74d518ed-d9e1-46f2-d637-c07f435e56c5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018334601,
          "user_tz": -540,
          "elapsed": 191726,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models))\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = [0.39676373 0.4084054 ]\n",
            "Epoch: 0002 cost = [0.08873104 0.09510502]\n",
            "Epoch: 0003 cost = [0.06583031 0.07203107]\n",
            "Epoch: 0004 cost = [0.05476968 0.05950908]\n",
            "Epoch: 0005 cost = [0.04954448 0.05296117]\n",
            "Epoch: 0006 cost = [0.04311784 0.04720076]\n",
            "Epoch: 0007 cost = [0.03801339 0.042494  ]\n",
            "Epoch: 0008 cost = [0.03488961 0.03926779]\n",
            "Epoch: 0009 cost = [0.03517963 0.03764357]\n",
            "Epoch: 0010 cost = [0.03135888 0.03439149]\n",
            "Epoch: 0011 cost = [0.02783976 0.03200202]\n",
            "Epoch: 0012 cost = [0.02782426 0.0295955 ]\n",
            "Epoch: 0013 cost = [0.02596771 0.02925884]\n",
            "Epoch: 0014 cost = [0.02572112 0.02772851]\n",
            "Epoch: 0015 cost = [0.02466742 0.02716377]\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c64lw1rHeTVL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8733a0e9-0962-4fd6-ae15-b923c9e9ca60",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018336220,
          "user_tz": -540,
          "elapsed": 1597,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.9889\n",
            "1 Accuracy: 0.9875\n",
            "Ensemble accuracy: 0.992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HdZXzrpcbFnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Does 'tf.layers' make negative effect?"
      ]
    },
    {
      "metadata": {
        "id": "sI_5lEXpcac8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model_TF:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
        "            # for testing\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "            # img 28x28x1 (black/white), Input Layer\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # Convolutional Layer #1\n",
        "            conv1 = tf.layers.conv2d(inputs=X_img, filters=8, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            # Pooling Layer #1\n",
        "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
        "                                         rate=0.7, training=self.training)\n",
        "\n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=16, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
        "                                         rate=0.7, training=self.training)\n",
        "\n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=32, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
        "                                         rate=0.7, training=self.training)\n",
        "                        \n",
        "            # Dense Layer #1 with Relu\n",
        "            flat = tf.reshape(dropout3, [-1, 32 * 4 * 4])\n",
        "            dense4 = tf.layers.dense(inputs=flat,\n",
        "                                     units=625, activation=tf.nn.relu,\n",
        "                                    kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
        "                                         rate=0.7, training=self.training)\n",
        "                                    \n",
        "            # Logits (no activation) Layer\n",
        "            self.logits = tf.layers.dense(inputs=dropout4, units=10,\n",
        "                                         kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits,\n",
        "                             feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy,\n",
        "                             feed_dict={self.X: x_test,\n",
        "                                        self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.training: training})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tqPkmaOohVzd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 16
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b31a7d68-cec7-45c5-8c48-4c229f25dc0b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018448397,
          "user_tz": -540,
          "elapsed": 111404,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "models_tf = []\n",
        "num_models = 2\n",
        "for m in range(num_models):\n",
        "    models_tf.append(Model_TF(sess, \"model_tf\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models_tf))\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models_tf):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Started!\n",
            "Epoch: 0001 cost = [1.58819899 1.72660743]\n",
            "Epoch: 0002 cost = [0.86550026 1.08872738]\n",
            "Epoch: 0003 cost = [0.73485713 0.89989919]\n",
            "Epoch: 0004 cost = [0.68126709 0.8060744 ]\n",
            "Epoch: 0005 cost = [0.64323365 0.74894865]\n",
            "Epoch: 0006 cost = [0.62050983 0.70693656]\n",
            "Epoch: 0007 cost = [0.59783467 0.67399316]\n",
            "Epoch: 0008 cost = [0.58686635 0.65314546]\n",
            "Epoch: 0009 cost = [0.57381208 0.63338927]\n",
            "Epoch: 0010 cost = [0.56233586 0.6198501 ]\n",
            "Epoch: 0011 cost = [0.55054288 0.60961522]\n",
            "Epoch: 0012 cost = [0.54562063 0.5964616 ]\n",
            "Epoch: 0013 cost = [0.53787615 0.58378026]\n",
            "Epoch: 0014 cost = [0.52385646 0.57304176]\n",
            "Epoch: 0015 cost = [0.51980004 0.56616407]\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QBgRceSQhWVn",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b8c7aee9-e45b-4417-e544-f10e51e5d49a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018449762,
          "user_tz": -540,
          "elapsed": 1342,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models_tf):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.8875\n",
            "1 Accuracy: 0.9299\n",
            "Ensemble accuracy: 0.9475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9lzmpiihk2WJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# How can we make it better?\n",
        "## 1. More filters"
      ]
    },
    {
      "metadata": {
        "id": "qqxvDg6xlUMX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model_F:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
        "            # for testing\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "            # img 28x28x1 (black/white), Input Layer\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # Convolutional Layer #1 and # Pooling Layer #1\n",
        "            weight1 = tf.Variable(tf.random_normal([3, 3, 1, 64], stddev=0.01))\n",
        "            layer1 = tf.nn.conv2d(X_img, weight1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer1 = tf.nn.relu(layer1)\n",
        "            layer1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer1 = tf.nn.dropout(layer1, keep_prob=0.7)\n",
        "\n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            weight2 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "            layer2 = tf.nn.conv2d(layer1, weight2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer2 = tf.nn.relu(layer2)\n",
        "            layer2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer2 = tf.nn.dropout(layer2, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            weight3 = tf.Variable(tf.random_normal([3, 3, 128, 256], stddev=0.01))\n",
        "            layer3 = tf.nn.conv2d(layer2, weight3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer3 = tf.nn.relu(layer3)\n",
        "            layer3 = tf.nn.max_pool(layer3, ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer3 = tf.nn.dropout(layer3, keep_prob=0.7)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(layer3, [-1, 256 * 4 * 4])\n",
        "            weight4 = tf.get_variable(\"weight4\", shape=[256 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias4 = tf.Variable(tf.random_normal([625]))\n",
        "            layer4 = tf.nn.relu(tf.matmul(flat, weight4) + bias4)\n",
        "            layer4 = tf.nn.dropout(layer4, keep_prob=0.7)\n",
        "            \n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
        "            weight5 = tf.get_variable(\"weight5\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias5 = tf.Variable(tf.random_normal([10]))\n",
        "            self.logits = tf.matmul(layer4, weight5) + bias5\n",
        "            \n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits,\n",
        "                             feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy,\n",
        "                             feed_dict={self.X: x_test,\n",
        "                                        self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.training: training})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zRJfiBd3mT7J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 16
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "a13151af-15b4-44a7-fb92-42c561fdb36d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018804095,
          "user_tz": -540,
          "elapsed": 353567,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "models_f = []\n",
        "num_models = 2\n",
        "for m in range(num_models):\n",
        "    models_f.append(Model_F(sess, \"model_f\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models_f))\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models_f):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Started!\n",
            "Epoch: 0001 cost = [0.32544126 0.29786262]\n",
            "Epoch: 0002 cost = [0.06812045 0.06596889]\n",
            "Epoch: 0003 cost = [0.04991813 0.05205958]\n",
            "Epoch: 0004 cost = [0.04232306 0.04355578]\n",
            "Epoch: 0005 cost = [0.03606108 0.03577516]\n",
            "Epoch: 0006 cost = [0.03309473 0.03263807]\n",
            "Epoch: 0007 cost = [0.02704002 0.02917156]\n",
            "Epoch: 0008 cost = [0.02642073 0.02689683]\n",
            "Epoch: 0009 cost = [0.0255812  0.02585493]\n",
            "Epoch: 0010 cost = [0.02282401 0.02258667]\n",
            "Epoch: 0011 cost = [0.02144546 0.02030435]\n",
            "Epoch: 0012 cost = [0.01980609 0.01776051]\n",
            "Epoch: 0013 cost = [0.01898248 0.02000082]\n",
            "Epoch: 0014 cost = [0.01830812 0.01828048]\n",
            "Epoch: 0015 cost = [0.01656955 0.01811374]\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qMm1pVtdpPUh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "40f02c3e-2c60-45e5-b70c-7b155c8afb8d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519018809079,
          "user_tz": -540,
          "elapsed": 4957,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models_f):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.9906\n",
            "1 Accuracy: 0.9915\n",
            "Ensemble accuracy: 0.9934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3V3CWkGvphbF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. More Convolution layers"
      ]
    },
    {
      "metadata": {
        "id": "cm4iynMFp08d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model_L:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
        "            # for testing\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "            # img 28x28x1 (black/white), Input Layer\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # Convolutional Layer #1 and # Pooling Layer #1\n",
        "            weight1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "            layer1 = tf.nn.conv2d(X_img, weight1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer1 = tf.nn.relu(layer1)\n",
        "            layer1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer1 = tf.nn.dropout(layer1, keep_prob=0.7)\n",
        "\n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            weight2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "            layer2 = tf.nn.conv2d(layer1, weight2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer2 = tf.nn.relu(layer2)\n",
        "            layer2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer2 = tf.nn.dropout(layer2, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            weight3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "            layer3 = tf.nn.conv2d(layer2, weight3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer3 = tf.nn.relu(layer3)\n",
        "            layer3 = tf.nn.max_pool(layer3, ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer3 = tf.nn.dropout(layer3, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #4 and Pooling Layer #4\n",
        "            weight4 = tf.Variable(tf.random_normal([3, 3, 128, 256], stddev=0.01))\n",
        "            layer4 = tf.nn.conv2d(layer3, weight4, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer4 = tf.nn.relu(layer4)\n",
        "            layer4 = tf.nn.max_pool(layer4, ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer4 = tf.nn.dropout(layer4, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #5 and Pooling Layer #5\n",
        "            weight5 = tf.Variable(tf.random_normal([3, 3, 256, 512], stddev=0.01))\n",
        "            layer5 = tf.nn.conv2d(layer4, weight5, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer5 = tf.nn.relu(layer5)\n",
        "            layer5 = tf.nn.max_pool(layer5, ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer5 = tf.nn.dropout(layer5, keep_prob=0.7)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(layer5, [-1, 512])\n",
        "            weight6 = tf.get_variable(\"weight6\", shape=[512, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias6 = tf.Variable(tf.random_normal([625]))\n",
        "            layer6 = tf.nn.relu(tf.matmul(flat, weight6) + bias6)\n",
        "            layer6 = tf.nn.dropout(layer6, keep_prob=0.7)\n",
        "            \n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
        "            weight7 = tf.get_variable(\"weight7\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias7 = tf.Variable(tf.random_normal([10]))\n",
        "            self.logits = tf.matmul(layer6, weight7) + bias7\n",
        "            \n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits,\n",
        "                             feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy,\n",
        "                             feed_dict={self.X: x_test,\n",
        "                                        self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.training: training})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lW_uQ9I5rRiB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 16
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "62113cf2-9c8c-4412-eaa6-954b2757be6f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519019134429,
          "user_tz": -540,
          "elapsed": 324613,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "models_l = []\n",
        "num_models = 2\n",
        "for m in range(num_models):\n",
        "    models_l.append(Model_L(sess, \"model_l\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models_l))\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models_l):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Started!\n",
            "Epoch: 0001 cost = [0.74765762 0.74498418]\n",
            "Epoch: 0002 cost = [0.11659466 0.11342738]\n",
            "Epoch: 0003 cost = [0.08305992 0.08312653]\n",
            "Epoch: 0004 cost = [0.07284182 0.07134266]\n",
            "Epoch: 0005 cost = [0.05952702 0.06136084]\n",
            "Epoch: 0006 cost = [0.05547855 0.05500782]\n",
            "Epoch: 0007 cost = [0.05089631 0.05127212]\n",
            "Epoch: 0008 cost = [0.04726947 0.04834025]\n",
            "Epoch: 0009 cost = [0.04451421 0.0481123 ]\n",
            "Epoch: 0010 cost = [0.04360798 0.04235404]\n",
            "Epoch: 0011 cost = [0.0427743  0.04128697]\n",
            "Epoch: 0012 cost = [0.03991457 0.04220425]\n",
            "Epoch: 0013 cost = [0.04090419 0.04104095]\n",
            "Epoch: 0014 cost = [0.03503438 0.03868317]\n",
            "Epoch: 0015 cost = [0.03787449 0.03759824]\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pcDdo6KmrT3d",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9c7808a8-9bcc-4024-d0e0-3a1d0c72209e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519019137883,
          "user_tz": -540,
          "elapsed": 3431,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models_l):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.987\n",
            "1 Accuracy: 0.982\n",
            "Ensemble accuracy: 0.9902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rdznlZnu4EDC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. More FC layers"
      ]
    },
    {
      "metadata": {
        "id": "1ckohi3_5qip",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model_DW:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
        "            # for testing\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "            # img 28x28x1 (black/white), Input Layer\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # Convolutional Layer #1 and # Pooling Layer #1\n",
        "            weight1 = tf.Variable(tf.random_normal([3, 3, 1, 16], stddev=0.01))\n",
        "            layer1 = tf.nn.conv2d(X_img, weight1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer1 = tf.nn.relu(layer1)\n",
        "            layer1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer1 = tf.nn.dropout(layer1, keep_prob=0.7)\n",
        "\n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            weight2 = tf.Variable(tf.random_normal([3, 3, 16, 32], stddev=0.01))\n",
        "            layer2 = tf.nn.conv2d(layer1, weight2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer2 = tf.nn.relu(layer2)\n",
        "            layer2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer2 = tf.nn.dropout(layer2, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            weight3 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "            layer3 = tf.nn.conv2d(layer2, weight3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer3 = tf.nn.relu(layer3)\n",
        "            layer3 = tf.nn.max_pool(layer3, ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer3 = tf.nn.dropout(layer3, keep_prob=0.7)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(layer3, [-1, 64 * 4 * 4])\n",
        "            weight4 = tf.get_variable(\"weight4\", shape=[64 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias4 = tf.Variable(tf.random_normal([625]))\n",
        "            layer4 = tf.nn.relu(tf.matmul(flat, weight4) + bias4)\n",
        "            layer4 = tf.nn.dropout(layer4, keep_prob=0.7)\n",
        "\n",
        "            # Dense Layer with Relu #2\n",
        "            weight5 = tf.get_variable(\"weight5\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias5 = tf.Variable(tf.random_normal([625]))\n",
        "            layer5 = tf.nn.relu(tf.matmul(layer4, weight5) + bias5)\n",
        "            layer5 = tf.nn.dropout(layer5, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #3\n",
        "            weight6 = tf.get_variable(\"weight6\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias6 = tf.Variable(tf.random_normal([625]))\n",
        "            layer6 = tf.nn.relu(tf.matmul(layer5, weight6) + bias6)\n",
        "            layer6 = tf.nn.dropout(layer6, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #4\n",
        "            weight7 = tf.get_variable(\"weight7\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias7 = tf.Variable(tf.random_normal([625]))\n",
        "            layer7 = tf.nn.relu(tf.matmul(layer6, weight7) + bias7)\n",
        "            layer7 = tf.nn.dropout(layer7, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #5\n",
        "            weight8 = tf.get_variable(\"weight8\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias8 = tf.Variable(tf.random_normal([625]))\n",
        "            layer8 = tf.nn.relu(tf.matmul(layer7, weight8) + bias8)\n",
        "            layer8 = tf.nn.dropout(layer8, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #6\n",
        "            weight9 = tf.get_variable(\"weight9\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias9 = tf.Variable(tf.random_normal([625]))\n",
        "            layer9 = tf.nn.relu(tf.matmul(layer8, weight9) + bias9)\n",
        "            layer9 = tf.nn.dropout(layer9, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #7\n",
        "            weight10 = tf.get_variable(\"weight10\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias10 = tf.Variable(tf.random_normal([625]))\n",
        "            layer10 = tf.nn.relu(tf.matmul(layer9, weight10) + bias10)\n",
        "            layer10 = tf.nn.dropout(layer10, keep_prob=0.7)\n",
        "            \n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
        "            weight11 = tf.get_variable(\"weight11\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias11 = tf.Variable(tf.random_normal([10]))\n",
        "            self.logits = tf.matmul(layer10, weight11) + bias11\n",
        "            \n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits,\n",
        "                             feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy,\n",
        "                             feed_dict={self.X: x_test,\n",
        "                                        self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.training: training})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYzBfoa678Bw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 6
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "69b4e3bd-b57f-4af2-cbcf-62eeaa600b08"
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "models_dw = []\n",
        "num_models = 2\n",
        "for m in range(num_models):\n",
        "    models_dw.append(Model_DW(sess, \"model_dw\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models_dw))\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models_dw):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Started!\n",
            "Epoch: 0001 cost = [1.22061955 1.24994143]\n",
            "Epoch: 0002 cost = [0.30474158 0.28040361]\n",
            "Epoch: 0003 cost = [0.22479764 0.2126325 ]\n",
            "Epoch: 0004 cost = [0.18707859 0.18094913]\n",
            "Epoch: 0005 cost = [0.16933504 0.16077038]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CUGk2Evt78b7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "26b6ac19-fb0b-412a-bcfa-742baa344cb2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518971776171,
          "user_tz": -540,
          "elapsed": 17380,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models_dw):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.9648\n",
            "1 Accuracy: 0.9653\n",
            "Ensemble accuracy: 0.9767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qSCFzddeRVCY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Why??"
      ]
    },
    {
      "metadata": {
        "id": "CzAlkFmd78zI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "0001ac89-270c-45d7-b657-102df882957e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518971866067,
          "user_tz": -540,
          "elapsed": 89736,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_size = len(mnist.train.labels)\n",
        "predictions = np.zeros([train_size, 10])\n",
        "for m_idx, m in enumerate(models_l):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.train.images, mnist.train.labels))\n",
        "    p = m.predict(mnist.train.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.train.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.97563636\n",
            "1 Accuracy: 0.97138184\n",
            "Ensemble accuracy: 0.98301816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FOoN6VpTYYKT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "dc44d46a-7b7b-4dbf-b964-5ebfed208f8b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518971956974,
          "user_tz": -540,
          "elapsed": 90648,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_size = len(mnist.train.labels)\n",
        "predictions = np.zeros([train_size, 10])\n",
        "for m_idx, m in enumerate(models_dw):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.train.images, mnist.train.labels))\n",
        "    p = m.predict(mnist.train.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.train.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.9623273\n",
            "1 Accuracy: 0.95927274\n",
            "Ensemble accuracy: 0.97443634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bj1RWG2Kc19A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Not overfitting "
      ]
    },
    {
      "metadata": {
        "id": "lwiIg4FpiR3H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Fast forward method?"
      ]
    },
    {
      "metadata": {
        "id": "IHmhumOCia32",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model_FF:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
        "            # for testing\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "            # img 28x28x1 (black/white), Input Layer\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # Convolutional Layer #1 and # Pooling Layer #1\n",
        "            weight1 = tf.Variable(tf.random_normal([3, 3, 1, 16], stddev=0.01))\n",
        "            layer1 = tf.nn.conv2d(X_img, weight1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer1 = tf.nn.relu(layer1)\n",
        "            layer1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer1 = tf.nn.dropout(layer1, keep_prob=0.7)\n",
        "\n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            weight2 = tf.Variable(tf.random_normal([3, 3, 16, 32], stddev=0.01))\n",
        "            layer2 = tf.nn.conv2d(layer1, weight2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer2 = tf.nn.relu(layer2)\n",
        "            layer2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer2 = tf.nn.dropout(layer2, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            weight3 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "            layer3 = tf.nn.conv2d(layer2, weight3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer3 = tf.nn.relu(layer3)\n",
        "            layer3 = tf.nn.max_pool(layer3, ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer3 = tf.nn.dropout(layer3, keep_prob=0.7)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(layer3, [-1, 64 * 4 * 4])\n",
        "            weight4 = tf.get_variable(\"weight4\", shape=[64 * 4 * 4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias4 = tf.Variable(tf.random_normal([625]))\n",
        "            layer4 = tf.nn.relu(tf.matmul(flat, weight4) + bias4)\n",
        "            layer4 = tf.nn.dropout(layer4, keep_prob=0.7)\n",
        "\n",
        "            # Dense Layer with Relu #2\n",
        "            weight5 = tf.get_variable(\"weight5\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias5 = tf.Variable(tf.random_normal([625]))\n",
        "            layer5 = tf.nn.relu(tf.matmul(layer4, weight5) + bias5)\n",
        "            layer5 = tf.nn.dropout(layer5, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #3\n",
        "            weight6 = tf.get_variable(\"weight6\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias6 = tf.Variable(tf.random_normal([625]))\n",
        "            layer6 = tf.nn.relu(tf.matmul(layer5, weight6) + bias6+layer4)\n",
        "            layer6 = tf.nn.dropout(layer6, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #4\n",
        "            weight7 = tf.get_variable(\"weight7\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias7 = tf.Variable(tf.random_normal([625]))\n",
        "            layer7 = tf.nn.relu(tf.matmul(layer6, weight7) + bias7)\n",
        "            layer7 = tf.nn.dropout(layer7, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #5\n",
        "            weight8 = tf.get_variable(\"weight8\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias8 = tf.Variable(tf.random_normal([625]))\n",
        "            layer8 = tf.nn.relu(tf.matmul(layer7, weight8) + bias8+layer6)\n",
        "            layer8 = tf.nn.dropout(layer8, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #6\n",
        "            weight9 = tf.get_variable(\"weight9\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias9 = tf.Variable(tf.random_normal([625]))\n",
        "            layer9 = tf.nn.relu(tf.matmul(layer8, weight9) + bias9)\n",
        "            layer9 = tf.nn.dropout(layer9, keep_prob=0.7)\n",
        "            \n",
        "            # Dense Layer with Relu #7\n",
        "            weight10 = tf.get_variable(\"weight10\", shape=[625, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias10 = tf.Variable(tf.random_normal([625]))\n",
        "            layer10 = tf.nn.relu(tf.matmul(layer9, weight10) + bias10+layer8)\n",
        "            layer10 = tf.nn.dropout(layer10, keep_prob=0.7)\n",
        "            \n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
        "            weight11 = tf.get_variable(\"weight11\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias11 = tf.Variable(tf.random_normal([10]))\n",
        "            self.logits = tf.matmul(layer10, weight11) + bias11\n",
        "            \n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits,\n",
        "                             feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy,\n",
        "                             feed_dict={self.X: x_test,\n",
        "                                        self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.training: training})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O1sRQN75qEOT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 6
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "9d2e0660-537f-46e9-9898-2d14bfc978e7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518972631362,
          "user_tz": -540,
          "elapsed": 673037,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "models_ff = []\n",
        "num_models = 2\n",
        "for m in range(num_models):\n",
        "    models_ff.append(Model_FF(sess, \"model_ff\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models_ff))\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models_ff):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Started!\n",
            "Epoch: 0001 cost = [1.08713569 1.10545502]\n",
            "Epoch: 0002 cost = [0.27936079 0.27878344]\n",
            "Epoch: 0003 cost = [0.20151371 0.20122847]\n",
            "Epoch: 0004 cost = [0.16855593 0.16655548]\n",
            "Epoch: 0005 cost = [0.1439016  0.14975275]\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BVygteM4qEnJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "abc18805-afc3-4534-b108-4a68b6c3d67d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518972649065,
          "user_tz": -540,
          "elapsed": 17664,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models_ff):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.9675\n",
            "1 Accuracy: 0.9669\n",
            "Ensemble accuracy: 0.9801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "70uIowZGttPu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## More Convolution layers+Fast forward"
      ]
    },
    {
      "metadata": {
        "id": "8ZK1nACmuC3c",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model_LP:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
        "            # for testing\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "            # img 28x28x1 (black/white), Input Layer\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # Convolutional Layer #1 and # Pooling Layer #1\n",
        "            weight1 = tf.Variable(tf.random_normal([3, 3, 1, 16], stddev=0.01))\n",
        "            layer1 = tf.nn.conv2d(X_img, weight1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer1 = tf.nn.relu(layer1)\n",
        "            layer1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer1 = tf.nn.dropout(layer1, keep_prob=0.7)\n",
        "\n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            weight2 = tf.Variable(tf.random_normal([3, 3, 16, 32], stddev=0.01))\n",
        "            layer2 = tf.nn.conv2d(layer1, weight2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer2 = tf.nn.relu(layer2)\n",
        "            layer2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer2 = tf.nn.dropout(layer2, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            weight3 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "            layer3 = tf.nn.conv2d(layer2, weight3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer3 = tf.nn.relu(layer3)\n",
        "            layer3 = tf.nn.max_pool(layer3, ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer3 = tf.nn.dropout(layer3, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #4 \n",
        "            weight4 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n",
        "            layer4 = tf.nn.conv2d(layer3, weight4, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer4 = tf.nn.relu(layer4)\n",
        "            layer4 = tf.nn.dropout(layer4, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #5 \n",
        "            weight5 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n",
        "            layer5 = tf.nn.conv2d(layer4, weight5, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer5 = tf.nn.relu(layer5)\n",
        "            layer5 = tf.nn.dropout(layer5, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #6 \n",
        "            weight6 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n",
        "            layer6 = tf.nn.conv2d(layer5, weight6, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer6 = tf.nn.relu(layer6+layer4)\n",
        "            layer6 = tf.nn.dropout(layer6, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #7 \n",
        "            weight7 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n",
        "            layer7 = tf.nn.conv2d(layer6, weight7, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer7 = tf.nn.relu(layer7)\n",
        "            layer7 = tf.nn.dropout(layer7, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #8 \n",
        "            weight8 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n",
        "            layer8 = tf.nn.conv2d(layer7, weight8, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer8 = tf.nn.relu(layer8+layer6)\n",
        "            layer8 = tf.nn.dropout(layer8, keep_prob=0.7)\n",
        "                        \n",
        "            # Convolutional Layer #9 \n",
        "            weight9 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n",
        "            layer9 = tf.nn.conv2d(layer8, weight9, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer9 = tf.nn.relu(layer9)\n",
        "            layer9 = tf.nn.dropout(layer9, keep_prob=0.7)            \n",
        "            \n",
        "            # Convolutional Layer #10 \n",
        "            weight10 = tf.Variable(tf.random_normal([3, 3, 64, 64], stddev=0.01))\n",
        "            layer10 = tf.nn.conv2d(layer9, weight10, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer10 = tf.nn.relu(layer10+layer8)\n",
        "            layer10 = tf.nn.dropout(layer10, keep_prob=0.7)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(layer10, [-1, 64*4*4])\n",
        "            weight11 = tf.get_variable(\"weight11\", shape=[64*4*4, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias11 = tf.Variable(tf.random_normal([625]))\n",
        "            layer11 = tf.nn.relu(tf.matmul(flat, weight11) + bias11)\n",
        "            layer11 = tf.nn.dropout(layer11, keep_prob=0.7)\n",
        "            \n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
        "            weight12 = tf.get_variable(\"weight12\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias12 = tf.Variable(tf.random_normal([10]))\n",
        "            self.logits = tf.matmul(layer11, weight12) + bias12\n",
        "            \n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits,\n",
        "                             feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy,\n",
        "                             feed_dict={self.X: x_test,\n",
        "                                        self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.training: training})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KxS3wk9CuDNi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 7
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "3b79b49b-587f-4ebd-b28b-cad3045e4a46",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519015478385,
          "user_tz": -540,
          "elapsed": 83455,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "models_lp = []\n",
        "num_models = 2\n",
        "for m in range(num_models):\n",
        "    models_lp.append(Model_LP(sess, \"model_lp\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models_lp))\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models_lp):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-7551a9ad9234>:103: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n",
            "Learning Started!\n",
            "Epoch: 0001 cost = [0.8657485  0.73944942]\n",
            "Epoch: 0002 cost = [0.19373159 0.18658945]\n",
            "Epoch: 0003 cost = [0.15164094 0.14569518]\n",
            "Epoch: 0004 cost = [0.13002215 0.12432395]\n",
            "Epoch: 0005 cost = [0.11443212 0.11445827]\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fejjPQX9uDeM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6870dc29-cd71-4707-e098-60791df60207",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519015486845,
          "user_tz": -540,
          "elapsed": 2494,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models_lp):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.9699\n",
            "1 Accuracy: 0.9668\n",
            "Ensemble accuracy: 0.9817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bflj2L1v5LWf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Final Model"
      ]
    },
    {
      "metadata": {
        "id": "VIc7SZ795Rwh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Model_Final:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
        "            # for testing\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "            # img 28x28x1 (black/white), Input Layer\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # Convolutional Layer #1 and # Pooling Layer #1\n",
        "            weight1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "            layer1 = tf.nn.conv2d(X_img, weight1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer1 = tf.nn.relu(layer1)\n",
        "            layer1 = tf.nn.max_pool(layer1, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer1 = tf.nn.dropout(layer1, keep_prob=0.7)\n",
        "\n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            weight2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "            layer2 = tf.nn.conv2d(layer1, weight2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer2 = tf.nn.relu(layer2)\n",
        "            layer2 = tf.nn.max_pool(layer2, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer2 = tf.nn.dropout(layer2, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            weight3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "            layer3 = tf.nn.conv2d(layer2, weight3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer3 = tf.nn.relu(layer3)\n",
        "            layer3 = tf.nn.max_pool(layer3, ksize=[1, 2, 2, 1], \n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer3 = tf.nn.dropout(layer3, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #4 \n",
        "            weight4 = tf.Variable(tf.random_normal([3, 3, 128, 128], stddev=0.01))\n",
        "            layer4 = tf.nn.conv2d(layer3, weight4, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer4 = tf.nn.relu(layer4)\n",
        "            layer4 = tf.nn.dropout(layer4, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #5 \n",
        "            weight5 = tf.Variable(tf.random_normal([3, 3, 128, 128], stddev=0.01))\n",
        "            layer5 = tf.nn.conv2d(layer4, weight5, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer5 = tf.nn.relu(layer5)\n",
        "            layer5 = tf.nn.dropout(layer5, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #6 \n",
        "            weight6 = tf.Variable(tf.random_normal([3, 3, 128, 128], stddev=0.01))\n",
        "            layer6 = tf.nn.conv2d(layer5, weight6, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer6 = tf.nn.relu(layer6+layer4)\n",
        "            layer6 = tf.nn.dropout(layer6, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #7 \n",
        "            weight7 = tf.Variable(tf.random_normal([3, 3, 128, 128], stddev=0.01))\n",
        "            layer7 = tf.nn.conv2d(layer6, weight7, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer7 = tf.nn.relu(layer7)\n",
        "            layer7 = tf.nn.dropout(layer7, keep_prob=0.7)\n",
        "            \n",
        "            # Convolutional Layer #8 \n",
        "            weight8 = tf.Variable(tf.random_normal([3, 3, 128, 128], stddev=0.01))\n",
        "            layer8 = tf.nn.conv2d(layer7, weight8, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer8 = tf.nn.relu(layer8+layer6)\n",
        "            layer8 = tf.nn.dropout(layer8, keep_prob=0.7)\n",
        "                        \n",
        "            # Convolutional Layer #9 \n",
        "            weight9 = tf.Variable(tf.random_normal([3, 3, 128, 256], stddev=0.01))\n",
        "            layer9 = tf.nn.conv2d(layer8, weight9, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer9 = tf.nn.relu(layer9)\n",
        "            layer9 = tf.nn.max_pool(layer9, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer9 = tf.nn.dropout(layer9, keep_prob=0.7)            \n",
        "            \n",
        "            # Convolutional Layer #10 \n",
        "            weight10 = tf.Variable(tf.random_normal([3, 3, 256, 512], stddev=0.01))\n",
        "            layer10 = tf.nn.conv2d(layer9, weight10, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            layer10 = tf.nn.relu(layer10)\n",
        "            layer10= tf.nn.max_pool(layer10, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            layer10 = tf.nn.dropout(layer10, keep_prob=0.7)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(layer10, [-1, 512])\n",
        "            weight11 = tf.get_variable(\"weight11\", shape=[512, 625],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias11 = tf.Variable(tf.random_normal([625]))\n",
        "            layer11 = tf.nn.relu(tf.matmul(flat, weight11) + bias11)\n",
        "            layer11 = tf.nn.dropout(layer11, keep_prob=0.7)\n",
        "            \n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
        "            weight12 = tf.get_variable(\"weight12\", shape=[625, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "            bias12 = tf.Variable(tf.random_normal([10]))\n",
        "            self.logits = tf.matmul(layer11, weight12) + bias12\n",
        "            \n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits,\n",
        "                             feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy,\n",
        "                             feed_dict={self.X: x_test,\n",
        "                                        self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.training: training})     \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ldoPhFQA5isB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 16
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "58a25bd5-f573-40d8-9e8d-625654a23cee",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519016406863,
          "user_tz": -540,
          "elapsed": 784174,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "models_final = []\n",
        "num_models = 3\n",
        "for m in range(num_models):\n",
        "    models_final.append(Model_Final(sess, \"model_final\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models_final))\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models_final):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Started!\n",
            "Epoch: 0001 cost = [1.36554919 1.0973608  1.30697067]\n",
            "Epoch: 0002 cost = [0.17076156 0.16019833 0.17053468]\n",
            "Epoch: 0003 cost = [0.11626267 0.11968032 0.11759452]\n",
            "Epoch: 0004 cost = [0.09714316 0.09965975 0.09466554]\n",
            "Epoch: 0005 cost = [0.08401598 0.09224376 0.08609654]\n",
            "Epoch: 0006 cost = [0.08168413 0.08350038 0.08075847]\n",
            "Epoch: 0007 cost = [0.07731145 0.0784253  0.07411046]\n",
            "Epoch: 0008 cost = [0.07196505 0.07656041 0.07018242]\n",
            "Epoch: 0009 cost = [0.06958004 0.07540338 0.06973924]\n",
            "Epoch: 0010 cost = [0.07013292 0.06981362 0.06568851]\n",
            "Epoch: 0011 cost = [0.06858541 0.07092512 0.06528341]\n",
            "Epoch: 0012 cost = [0.06579207 0.07098167 0.06443044]\n",
            "Epoch: 0013 cost = [0.06240419 0.06748101 0.06027985]\n",
            "Epoch: 0014 cost = [0.06454814 0.06427529 0.05974082]\n",
            "Epoch: 0015 cost = [0.06161656 0.06430966 0.05685076]\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "smvtiRDF5shR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 4
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f128a442-3cc0-489b-83d7-cda2a887b033",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1519016436822,
          "user_tz": -540,
          "elapsed": 6906,
          "user": {
            "displayName": "박하림",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "110181269539721203399"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models_final):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Accuracy: 0.9791\n",
            "1 Accuracy: 0.9841\n",
            "2 Accuracy: 0.9833\n",
            "Ensemble accuracy: 0.9922\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}